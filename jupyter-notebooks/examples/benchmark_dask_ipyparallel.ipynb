{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet of code properly adds the working source root path to python's path\n",
    "# so you no longer have to install spykshrk through setuptools\n",
    "import sys, os\n",
    "root_depth = 2\n",
    "notebook_dir = globals()['_dh'][0]\n",
    "root_path = os.path.abspath(os.path.join(notebook_dir, '../'*root_depth))\n",
    "# Add to python's path\n",
    "try:\n",
    "    while True:\n",
    "        sys.path.remove(root_path)\n",
    "except ValueError:\n",
    "    # no more root paths\n",
    "    pass\n",
    "sys.path.append(root_path)\n",
    "# Alternatively set root path as current working directory\n",
    "#os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask.base import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipyparallel as ipp\n",
    "import functools\n",
    "import time\n",
    "import h5py\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "\n",
    "from spykshrk.util import Groupby\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "#dask.set_options(get=dask.multiprocessing.get)\n",
    "\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "\n",
    "delay_counter = int(1e4)\n",
    "\n",
    "def mean_func(data):\n",
    "    import numpy as np\n",
    "    return np.mean(data, axis=0)\n",
    "\n",
    "def custom_mean_func(data):\n",
    "    import numpy as np\n",
    "    return np.mean(data, axis=1, keepdims=True)\n",
    "\n",
    "def wait_func(data):\n",
    "    import numpy as np\n",
    "    global delay_counter\n",
    "    for ii in range(delay_counter*np.prod(data.shape)):\n",
    "        j = ii + ii\n",
    "    return np.mean(data, axis=0)\n",
    "\n",
    "def custom_wait_func(data):\n",
    "    import numpy as np\n",
    "    global delay_counter\n",
    "    for ii in range(delay_counter*np.prod(data.shape)):\n",
    "        j = ii + ii\n",
    "    return np.mean(data, axis=1)\n",
    "\n",
    "test_func = mean_func\n",
    "custom_test_func = custom_mean_func\n",
    "#test_func = wait_func\n",
    "#custom_test_func = custom_wait_func\n",
    "dview.push(dict(test_func=test_func))\n",
    "dview.push(dict(custom_test_func=custom_test_func))\n",
    "dview.push(dict(delay_counter=delay_counter))\n",
    "\n",
    "\n",
    "data_mag = int(1e6)\n",
    "bin_size = int(1e5)\n",
    "data_size = 8\n",
    "count1 = pd.DataFrame(np.arange(data_mag), columns=['first'])\n",
    "count2 = pd.DataFrame((np.arange(data_mag)/bin_size).astype('int'), columns=['second'])\n",
    "data_col = pd.DataFrame(np.ones([data_mag, data_size]), columns=[str(ii) for ii in range(data_size)])\n",
    "\n",
    "test_df = count1.join([count2, data_col])\n",
    "\n",
    "test_df.to_hdf('/tmp/tmp.h5', key='/a', format='table')\n",
    "\n",
    "with h5py.File('/tmp/tmp_np.h5', 'w') as f:\n",
    "    grp_list = []\n",
    "    for grp_id, grp in test_df.groupby('second'):\n",
    "        grp_list.append(grp.values)\n",
    "\n",
    "    f.create_dataset('a', data=np.array(grp_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Test dask.DataFrame parallelization with in memory data source\n",
    "test_dask = dd.from_pandas(test_df, npartitions=10)\n",
    "dask_frame_task = test_dask.groupby('second').apply(test_func, meta=test_dask)\n",
    "dask_frame_results = dask_frame_task.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def func1(s):\n",
    "    if test_func is wait_func:\n",
    "        for ii in range(delay_counter*s.size().values[0]):\n",
    "            j = ii + ii\n",
    "    return s.sum(), s.count()\n",
    "\n",
    "def func2(sum, count):\n",
    "    return sum.sum(), count.sum()\n",
    "\n",
    "def func3(sum, count):\n",
    "    return sum / count\n",
    "\n",
    "custom_mean = dd.Aggregation('custom_mean', func1, func2, func3)\n",
    "\n",
    "test_dask2 = dd.from_pandas(test_df, npartitions=10)\n",
    "\n",
    "dask_agg_task = test_dask2.groupby('second').agg(custom_mean)\n",
    "\n",
    "dask_agg_results = dask_agg_task.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_agg_task.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def func1(s):\n",
    "    if test_func is wait_func:\n",
    "        for ii in range(delay_counter*s.size().values[0]):\n",
    "            j = ii + ii\n",
    "    return s.sum(), s.count()\n",
    "\n",
    "def func2(sum, count):\n",
    "    return sum.sum(), count.sum()\n",
    "\n",
    "def func3(sum, count):\n",
    "    return sum / count\n",
    "\n",
    "custom_mean = dd.Aggregation('custom_mean', func1, func2, func3)\n",
    "\n",
    "dask_file1 = dd.read_hdf('/tmp/tmp.h5', key='/a', chunksize=int(1e4))\n",
    "\n",
    "dask_file_agg_task = dask_file1.groupby('second').agg(custom_mean)\n",
    "\n",
    "dask_agg_results = dask_file_agg_task.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_file_agg_task.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Test dask.DataFrame parallelization with file data source\n",
    "dask_file2 = dd.read_hdf('/tmp/tmp.h5', key='/a', chunksize=int(1e4))\n",
    "dask_file_frame_task = dask_file2.groupby('second').apply(test_func, meta=dask_frame_results)\n",
    "dask_file_frame_results = dask_file_frame_task.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dask_file_frame_task.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def delayed_hdf5(path, dset, lo,hi):\n",
    "    with h5py.File(path, mode='r',) as f:\n",
    "        arr = f[dset][lo:hi]\n",
    "    return arr\n",
    "\n",
    "def daskify_hdf5_1d_array_delayed(\n",
    "        path,\n",
    "        dset,\n",
    "        chunksize=int(1e6),\n",
    "    ):\n",
    "    \n",
    "    with h5py.File(path, mode='r') as f:\n",
    "        size = f[dset].shape\n",
    "        dtype = f[dset].dtype\n",
    "    \n",
    "    chunk_edges = np.arange(0, size[0], chunksize)\n",
    "    \n",
    "    if chunk_edges[-1] != size[0]:\n",
    "        chunk_edges = np.r_[chunk_edges, size[0]]\n",
    "        \n",
    "    arrs = [da.from_delayed(\n",
    "                dask.delayed(delayed_hdf5)(path, dset, lo, hi),\n",
    "                (hi-lo, size[1], size[2]),\n",
    "                dtype\n",
    "            )\n",
    "            for lo,hi in zip(chunk_edges[:-1],chunk_edges[1:])\n",
    "           ]\n",
    "    arr = da.concatenate(arrs)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "dask_file_arr = daskify_hdf5_1d_array_delayed('/tmp/tmp_np.h5', 'a', chunksize=1)\n",
    "dask_file_arr_task = da.apply_along_axis(test_func, arr=dask_file_arr, axis=1)\n",
    "dask_file_arr_results = dask_file_arr_task.compute()\n",
    "#dask_file_arr.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_file_arr_task.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Test dask.Array parallelization with in memory data source\n",
    "grp_obj1 = Groupby(test_df.values, test_df['second'])\n",
    "array_data1 = np.array(list(grp_obj1))\n",
    "dask_array1 = da.from_array(array_data1, chunks=(data_mag/bin_size/10, bin_size, data_size+2))\n",
    "dask_array_task1 = da.apply_along_axis(test_func, arr=dask_array1, axis=1)\n",
    "# task = da.apply_along_axis(np.mean, arr=task, axis=1)\n",
    "dask_array_results1 = dask_array_task1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array_task1.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Test dask.Array mean parallelization\n",
    "grp_obj2 = Groupby(test_df.values, test_df['second'])\n",
    "dask_array2 = da.from_array(np.array(list(grp_obj2)), chunks=(data_mag/bin_size/10, bin_size, data_size+2))\n",
    "dask_array_mean_results = dask_array2.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grp_obj3 = Groupby(test_df.values, test_df['second'])\n",
    "\n",
    "def combine_grp_results(results_list):\n",
    "    return np.array(results_list)\n",
    "\n",
    "delay_result_list = []\n",
    "for grp_entry in grp_obj3:\n",
    "    delay_result_list.append(dask.delayed(custom_test_func)(grp_entry))\n",
    "\n",
    "delay_task = dask.delayed(combine_grp_results)(delay_result_list)\n",
    "\n",
    "delay_results = delay_task.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Test custom function dask.Array parallelization\n",
    "\n",
    "def benchmark_mean(v):\n",
    "    \n",
    "    name = 'benchmark_mean-' + tokenize(v)\n",
    "    new_chunk = [v.chunks[0], v.chunks[1], v.chunks[2]]\n",
    "    dsk = {(name, i,j, k): (custom_test_func, (v.name, i, j, k))\n",
    "           for i in range(len(v.chunks[0]))\n",
    "           for j in range(len(v.chunks[1]))\n",
    "           for k in range(len(v.chunks[2]))}\n",
    "    dsk.update(v.dask)\n",
    "    dtype = v.dtype\n",
    "    \n",
    "    return da.Array(dsk, name, new_chunk, dtype)\n",
    "\n",
    "grp_obj3 = Groupby(test_df.values, test_df['second'])\n",
    "dask_array3 = da.from_array(np.array(list(grp_obj3)), chunks=(data_mag/bin_size/10, bin_size, data_size+2))\n",
    "custom_dask_array_task = benchmark_mean(dask_array3)\n",
    "custom_dask_array_results = custom_dask_array_task.compute()\n",
    "custom_dask_array_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.dot import dot_graph\n",
    "dot_graph(custom_dask_array_task.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grp_obj = Groupby(test_df.values, test_df['second'])\n",
    "new_grp = np.array(list(grp_obj))\n",
    "ipp_results = np.array(dview.map_sync(test_func, new_grp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def ipp_file_test(rank):\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    with h5py.File('/tmp/tmp_np.h5', 'r') as f:\n",
    "        data = f.get('a')\n",
    "        sub_data = data[rank,:,:]\n",
    "        return test_func(sub_data)\n",
    "\n",
    "ipp_file_results = np.array(dview.map_sync(ipp_file_test, range(10)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def ipp_grp_test(grp_entry):\n",
    "    import numpy as np\n",
    "    grp_id, df = grp_entry\n",
    "    return test_func(df)\n",
    "\n",
    "pd_grp = test_df.groupby('second')\n",
    "pd_ipp_results = np.array(dview.map_sync(ipp_grp_test, pd_grp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grp = test_df.groupby('second')\n",
    "pd_results1 = grp.apply(test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grp = test_df.groupby('second')\n",
    "pd_results2 = grp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    for ii in range(delay_counter*np.prod(test_df.shape)):\n",
    "        j = ii + ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}